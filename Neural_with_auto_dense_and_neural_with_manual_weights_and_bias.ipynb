{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path):\n",
    "    \n",
    "    val = pd.read_csv(path)\n",
    "    \n",
    "    val.drop(columns=['PassengerId','Name','Ticket','Pclass','Fare'], inplace=True)\n",
    "    val.replace(['male','female'], [1,0],inplace=True)\n",
    "    \n",
    "    embar_val = pd.DataFrame({'S' : val.Embarked == 'S','C' : val.Embarked == 'C','Q' : val.Embarked == 'Q','NAN' : val.Embarked == 'nan'}).astype(int)\n",
    "    val = val.drop('Embarked',axis=1)\n",
    "    \n",
    "    alone = pd.DataFrame({'alone' : val.Parch + val.SibSp == 0}).astype(int)\n",
    "    #val = val.drop(['Parch','SibSp'],axis=1)\n",
    "    val = pd.concat([val,embar_val,alone], axis=1, join='inner')\n",
    "    val['Cabin'].fillna('X', inplace = True)\n",
    "    val['Cabin_encoded'] = val['Cabin'].apply(lambda x:x[0] if len(x) > 1 else x)\n",
    "    \n",
    "    t = pd.DataFrame({'X' : val.Cabin_encoded == 'X','C' : val.Cabin_encoded == 'C','E' : val.Cabin_encoded == 'E','G':val.Cabin_encoded == 'G','D':val.Cabin_encoded == 'D','A': val.Cabin_encoded == 'A','B': val.Cabin_encoded == 'B','F': val.Cabin_encoded == 'F','T':val.Cabin_encoded == 'T'}).astype(int)\n",
    "    val.Age.fillna(0,inplace=True)\n",
    "    val = pd.concat([val, t], axis=1, join='inner')\n",
    "    val = val.drop(['Cabin_encoded','Cabin'],axis=1)\n",
    "    \n",
    "    if 'Survived' in val.columns:\n",
    "        global survive\n",
    "        survive = val.Survived\n",
    "        val.drop('Survived', inplace=True, axis=1)\n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocess('data/train.csv')\n",
    "train.head()\n",
    "\n",
    "test = preprocess('data/test.csv')\n",
    "y_test = pd.read_csv('data/surv_test.csv').iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaourab/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/gaourab/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n",
      "/home/gaourab/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n",
      "/home/gaourab/anaconda3/lib/python3.5/site-packages/ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "train_x = train.as_matrix()\n",
    "train_y = pd.get_dummies(survive).as_matrix()\n",
    "\n",
    "test_x = test.as_matrix()\n",
    "test_y = pd.get_dummies(y_test).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 18)\n",
      "(418, 2)\n",
      "(891, 18)\n",
      "(891, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(test_x))\n",
    "print(np.shape(test_y))\n",
    "\n",
    "print(np.shape(train_x))\n",
    "print(np.shape(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_n = 1000\n",
    "class_n = 2\n",
    "bathc_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None,18])\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_model(x):\n",
    "    \n",
    "    layer1 = tf.layers.dense(x, units=100, activation=tf.nn.relu)\n",
    "    layer2 = tf.layers.dense(layer1, units=200, activation=tf.nn.relu)\n",
    "    drop = tf.layers.dropout(layer2, rate=0.5)\n",
    "    out = tf.layers.dense(drop, units=class_n)\n",
    "    \n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-02d6f9b11280>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = n_model(x)\n",
    "\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(labels=train_y, logits=pred)\n",
    "optimize = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "corr_pred = tf.equal(tf.argmax(pred,axis=1), tf.argmax(y, axis=1))\n",
    "acc = tf.reduce_mean(tf.cast(corr_pred, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5598086\n",
      "0.36842105\n",
      "0.39473686\n",
      "0.47368422\n",
      "0.6650718\n",
      "0.64593303\n",
      "0.638756\n",
      "0.638756\n",
      "0.6411483\n",
      "0.6483254\n",
      "0.5861244\n",
      "0.46411484\n",
      "0.46411484\n",
      "0.59090906\n",
      "0.65311\n",
      "0.6363636\n",
      "0.6363636\n",
      "0.6363636\n",
      "0.6363636\n",
      "0.64593303\n",
      "0.7200957\n",
      "0.81100476\n",
      "0.80861247\n",
      "0.791866\n",
      "0.6985646\n",
      "0.6602871\n",
      "0.65311\n",
      "0.6698565\n",
      "0.7320574\n",
      "0.8181818\n",
      "0.78708136\n",
      "0.8014354\n",
      "0.77272725\n",
      "0.7583732\n",
      "0.7751196\n",
      "0.78708136\n",
      "0.784689\n",
      "0.78229666\n",
      "0.76555026\n",
      "0.791866\n",
      "0.784689\n",
      "0.77751195\n",
      "0.77751195\n",
      "0.78229666\n",
      "0.7751196\n",
      "0.7751196\n",
      "0.7703349\n",
      "0.77272725\n",
      "0.77751195\n",
      "0.76555026\n",
      "0.7631579\n",
      "0.7679426\n",
      "0.7703349\n",
      "0.7583732\n",
      "0.76076555\n",
      "0.76076555\n",
      "0.7583732\n",
      "0.7679426\n",
      "0.7703349\n",
      "0.77272725\n",
      "0.78229666\n",
      "0.78229666\n",
      "0.78229666\n",
      "0.78229666\n",
      "0.784689\n",
      "0.79425836\n",
      "0.7894737\n",
      "0.8181818\n",
      "0.82535887\n",
      "0.83253586\n",
      "0.8779904\n",
      "0.9019139\n",
      "0.9019139\n",
      "0.90909094\n",
      "0.9066986\n",
      "0.9114832\n",
      "0.9138756\n",
      "0.91626793\n",
      "0.923445\n",
      "0.92105263\n",
      "0.9282297\n",
      "0.923445\n",
      "0.9282297\n",
      "0.92583734\n",
      "0.9282297\n",
      "0.930622\n",
      "0.923445\n",
      "0.930622\n",
      "0.9282297\n",
      "0.930622\n",
      "0.93301433\n",
      "0.93301433\n",
      "0.930622\n",
      "0.930622\n",
      "0.9282297\n",
      "0.9282297\n",
      "0.9282297\n",
      "0.92583734\n",
      "0.930622\n",
      "0.9282297\n",
      "0.930622\n",
      "0.930622\n",
      "0.9282297\n",
      "0.930622\n",
      "0.923445\n",
      "0.93301433\n",
      "0.923445\n",
      "0.91626793\n",
      "0.9066986\n",
      "0.91626793\n",
      "0.90430623\n",
      "0.9114832\n",
      "0.8995215\n",
      "0.9114832\n",
      "0.90909094\n",
      "0.8995215\n",
      "0.90909094\n",
      "0.8971292\n",
      "0.9066986\n",
      "0.8899522\n",
      "0.9066986\n",
      "0.9066986\n",
      "0.8971292\n",
      "0.9138756\n",
      "0.8971292\n",
      "0.90430623\n",
      "0.9138756\n",
      "0.8971292\n",
      "0.9114832\n",
      "0.9114832\n",
      "0.8971292\n",
      "0.9114832\n",
      "0.90430623\n",
      "0.9019139\n",
      "0.90909094\n",
      "0.9019139\n",
      "0.9066986\n",
      "0.90909094\n",
      "0.8995215\n",
      "0.90909094\n",
      "0.90430623\n",
      "0.8947368\n",
      "0.9114832\n",
      "0.88755983\n",
      "0.9138756\n",
      "0.8971292\n",
      "0.8947368\n",
      "0.9114832\n",
      "0.8947368\n",
      "0.9066986\n",
      "0.90909094\n",
      "0.8899522\n",
      "0.9066986\n",
      "0.9066986\n",
      "0.8923445\n",
      "0.90909094\n",
      "0.9066986\n",
      "0.9019139\n",
      "0.90909094\n",
      "0.8923445\n",
      "0.90430623\n",
      "0.9066986\n",
      "0.8923445\n",
      "0.90430623\n",
      "0.90430623\n",
      "0.8947368\n",
      "0.90430623\n",
      "0.8947368\n",
      "0.90430623\n",
      "0.90430623\n",
      "0.8947368\n",
      "0.90430623\n",
      "0.9019139\n",
      "0.8971292\n",
      "0.9066986\n",
      "0.8947368\n",
      "0.9019139\n",
      "0.9019139\n",
      "0.8971292\n",
      "0.90430623\n",
      "0.8947368\n",
      "0.9019139\n",
      "0.9019139\n",
      "0.8971292\n",
      "0.90430623\n",
      "0.8971292\n",
      "0.9019139\n",
      "0.9019139\n",
      "0.8995215\n",
      "0.90430623\n",
      "0.8923445\n",
      "0.90430623\n",
      "0.8899522\n",
      "0.9066986\n",
      "0.8971292\n",
      "0.90430623\n",
      "0.9019139\n",
      "0.8923445\n",
      "0.9066986\n",
      "0.8899522\n",
      "0.9114832\n",
      "0.88755983\n",
      "0.9066986\n",
      "0.8995215\n",
      "0.8923445\n",
      "0.90430623\n",
      "0.88755983\n",
      "0.90430623\n",
      "0.8971292\n",
      "0.9019139\n",
      "0.9066986\n",
      "0.8923445\n",
      "0.90430623\n",
      "0.8923445\n",
      "0.90430623\n",
      "0.8995215\n",
      "0.8995215\n",
      "0.9066986\n",
      "0.8971292\n",
      "0.90430623\n",
      "0.9019139\n",
      "0.9019139\n",
      "0.8995215\n",
      "0.8971292\n",
      "0.90430623\n",
      "0.8899522\n",
      "0.90430623\n",
      "0.8947368\n",
      "0.90430623\n",
      "0.9019139\n",
      "0.8995215\n",
      "0.90430623\n",
      "0.8947368\n",
      "0.90430623\n",
      "0.8923445\n",
      "0.9019139\n",
      "0.8947368\n",
      "0.8995215\n",
      "0.8971292\n",
      "0.8971292\n",
      "0.9019139\n",
      "0.8947368\n",
      "0.90430623\n",
      "0.8899522\n",
      "0.9066986\n",
      "0.8851675\n",
      "0.9066986\n",
      "0.8899522\n",
      "0.9019139\n",
      "0.9019139\n",
      "0.8923445\n",
      "0.9066986\n",
      "0.8851675\n",
      "0.9019139\n",
      "0.8899522\n",
      "0.9019139\n",
      "0.8971292\n",
      "0.8851675\n",
      "0.8923445\n",
      "0.8779904\n",
      "0.90430623\n",
      "0.8995215\n",
      "0.8899522\n",
      "0.8971292\n",
      "0.88755983\n",
      "0.8995215\n",
      "0.9019139\n",
      "0.88755983\n",
      "0.8971292\n",
      "0.8923445\n",
      "0.8947368\n",
      "0.90430623\n",
      "0.88755983\n",
      "0.9019139\n",
      "0.9019139\n",
      "0.8899522\n",
      "0.8995215\n",
      "0.8947368\n",
      "0.8947368\n",
      "0.9019139\n",
      "0.8899522\n",
      "0.8995215\n",
      "0.8971292\n",
      "0.88755983\n",
      "0.8995215\n",
      "0.88755983\n",
      "0.8971292\n",
      "0.8971292\n",
      "0.8923445\n",
      "0.8995215\n",
      "0.8923445\n",
      "0.8923445\n",
      "0.88755983\n",
      "0.88755983\n",
      "0.8971292\n",
      "0.8923445\n",
      "0.90430623\n",
      "0.8923445\n",
      "0.88755983\n",
      "0.8923445\n",
      "0.8899522\n",
      "0.8971292\n",
      "0.88755983\n",
      "0.8923445\n",
      "0.8971292\n",
      "0.8899522\n",
      "0.8995215\n",
      "0.88755983\n",
      "0.8971292\n",
      "0.8851675\n",
      "0.8923445\n",
      "0.88755983\n",
      "0.8947368\n",
      "0.8803828\n",
      "0.8947368\n",
      "0.8803828\n",
      "0.8923445\n",
      "0.8851675\n",
      "0.8947368\n",
      "0.8899522\n",
      "0.8923445\n",
      "0.8899522\n",
      "0.88755983\n",
      "0.88755983\n",
      "0.88755983\n",
      "0.8899522\n",
      "0.88755983\n",
      "0.8899522\n",
      "0.8899522\n",
      "0.88755983\n",
      "0.8899522\n",
      "0.88755983\n",
      "0.8923445\n",
      "0.8803828\n",
      "0.8947368\n",
      "0.8732057\n",
      "0.8947368\n",
      "0.8755981\n",
      "0.8971292\n",
      "0.88755983\n",
      "0.8899522\n",
      "0.8851675\n",
      "0.8851675\n",
      "0.8827751\n",
      "0.88755983\n",
      "0.8899522\n",
      "0.88755983\n",
      "0.88755983\n",
      "0.88755983\n",
      "0.8827751\n",
      "0.88755983\n",
      "0.8851675\n",
      "0.8851675\n",
      "0.8851675\n",
      "0.8851675\n",
      "0.8851675\n",
      "0.88755983\n",
      "0.8827751\n",
      "0.8851675\n",
      "0.8827751\n",
      "0.8827751\n",
      "0.8851675\n",
      "0.88755983\n",
      "0.8851675\n",
      "0.8851675\n",
      "0.8827751\n",
      "0.8827751\n",
      "0.8755981\n",
      "0.8851675\n",
      "0.8684211\n",
      "0.8827751\n",
      "0.8779904\n",
      "0.8851675\n",
      "0.8827751\n",
      "0.8803828\n",
      "0.8851675\n",
      "0.8684211\n",
      "0.88755983\n",
      "0.8851675\n",
      "0.8851675\n",
      "0.8779904\n",
      "0.8755981\n",
      "0.8803828\n",
      "0.8827751\n",
      "0.8851675\n",
      "0.8779904\n",
      "0.8755981\n",
      "0.8732057\n",
      "0.8827751\n",
      "0.8803828\n",
      "0.8827751\n",
      "0.8827751\n",
      "0.8827751\n",
      "0.8851675\n",
      "0.8779904\n",
      "0.8827751\n",
      "0.8827751\n",
      "0.8827751\n",
      "0.8827751\n",
      "0.8803828\n",
      "0.8827751\n",
      "0.8851675\n",
      "0.8803828\n",
      "0.8827751\n",
      "0.8827751\n",
      "0.8827751\n",
      "0.8803828\n",
      "0.8827751\n",
      "0.8851675\n",
      "0.88755983\n",
      "0.8827751\n",
      "0.8803828\n",
      "0.8827751\n",
      "0.8827751\n",
      "0.8827751\n",
      "0.8827751\n",
      "0.8827751\n",
      "0.8827751\n",
      "0.8827751\n",
      "0.8851675\n",
      "0.8851675\n",
      "0.8827751\n",
      "0.8803828\n",
      "0.8803828\n",
      "0.8827751\n",
      "0.8851675\n",
      "0.8803828\n",
      "0.8827751\n",
      "0.8803828\n",
      "0.8803828\n",
      "0.8827751\n",
      "0.8803828\n",
      "0.8755981\n",
      "0.8779904\n",
      "0.8755981\n",
      "0.8803828\n",
      "0.8755981\n",
      "0.8803828\n",
      "0.8732057\n",
      "0.8803828\n",
      "0.87081337\n",
      "0.8779904\n",
      "0.8732057\n",
      "0.8827751\n",
      "0.8684211\n",
      "0.8779904\n",
      "0.8684211\n",
      "0.8803828\n",
      "0.87081337\n",
      "0.8779904\n",
      "0.8732057\n",
      "0.8779904\n",
      "0.8779904\n",
      "0.8803828\n",
      "0.8779904\n",
      "0.8732057\n",
      "0.8803828\n",
      "0.87081337\n",
      "0.8755981\n",
      "0.8660287\n",
      "0.8827751\n",
      "0.8636364\n",
      "0.8803828\n",
      "0.8660287\n",
      "0.8732057\n",
      "0.87081337\n",
      "0.8755981\n",
      "0.8732057\n",
      "0.8732057\n",
      "0.8779904\n",
      "0.8732057\n",
      "0.8779904\n",
      "0.87081337\n",
      "0.8755981\n",
      "0.8636364\n",
      "0.8732057\n",
      "0.8660287\n",
      "0.8779904\n",
      "0.8660287\n",
      "0.8779904\n",
      "0.87081337\n",
      "0.8755981\n",
      "0.87081337\n",
      "0.8779904\n",
      "0.87081337\n",
      "0.8755981\n",
      "0.87081337\n",
      "0.8660287\n",
      "0.8660287\n",
      "0.8684211\n",
      "0.8732057\n",
      "0.87081337\n",
      "0.8755981\n",
      "0.8732057\n",
      "0.8732057\n",
      "0.8660287\n",
      "0.8732057\n",
      "0.8516746\n",
      "0.8755981\n",
      "0.8516746\n",
      "0.8732057\n",
      "0.8684211\n",
      "0.8732057\n",
      "0.8660287\n",
      "0.87081337\n",
      "0.8732057\n",
      "0.8732057\n",
      "0.87081337\n",
      "0.861244\n",
      "0.87081337\n",
      "0.8516746\n",
      "0.8732057\n",
      "0.85406697\n",
      "0.8755981\n",
      "0.8564593\n",
      "0.8660287\n",
      "0.8684211\n",
      "0.8684211\n",
      "0.87081337\n",
      "0.8516746\n",
      "0.87081337\n",
      "0.8516746\n",
      "0.8755981\n",
      "0.84689\n",
      "0.87081337\n",
      "0.8684211\n",
      "0.8636364\n",
      "0.8732057\n",
      "0.85406697\n",
      "0.87081337\n",
      "0.8516746\n",
      "0.87081337\n",
      "0.87081337\n",
      "0.861244\n",
      "0.8684211\n",
      "0.8516746\n",
      "0.8684211\n",
      "0.87081337\n",
      "0.8684211\n",
      "0.8684211\n",
      "0.8564593\n",
      "0.8660287\n",
      "0.8588517\n",
      "0.8660287\n",
      "0.8660287\n",
      "0.861244\n",
      "0.8660287\n",
      "0.861244\n",
      "0.861244\n",
      "0.861244\n",
      "0.861244\n",
      "0.8660287\n",
      "0.8660287\n",
      "0.861244\n",
      "0.8636364\n",
      "0.8588517\n",
      "0.8636364\n",
      "0.861244\n",
      "0.8660287\n",
      "0.861244\n",
      "0.8660287\n",
      "0.861244\n",
      "0.861244\n",
      "0.861244\n",
      "0.8588517\n",
      "0.8588517\n",
      "0.8588517\n",
      "0.8588517\n",
      "0.8588517\n",
      "0.8588517\n",
      "0.861244\n",
      "0.8588517\n",
      "0.85406697\n",
      "0.8588517\n",
      "0.8588517\n",
      "0.8588517\n",
      "0.85406697\n",
      "0.861244\n",
      "0.85406697\n",
      "0.8588517\n",
      "0.8492823\n",
      "0.8588517\n",
      "0.85406697\n",
      "0.8516746\n",
      "0.8588517\n",
      "0.85406697\n",
      "0.8564593\n",
      "0.8516746\n",
      "0.8588517\n",
      "0.8516746\n",
      "0.861244\n",
      "0.85406697\n",
      "0.85406697\n",
      "0.8564593\n",
      "0.8492823\n",
      "0.85406697\n",
      "0.8492823\n",
      "0.861244\n",
      "0.8516746\n",
      "0.85406697\n",
      "0.8492823\n",
      "0.8516746\n",
      "0.85406697\n",
      "0.8516746\n",
      "0.8588517\n",
      "0.85406697\n",
      "0.8588517\n",
      "0.85406697\n",
      "0.85406697\n",
      "0.8516746\n",
      "0.8516746\n",
      "0.8492823\n",
      "0.8516746\n",
      "0.85406697\n",
      "0.85406697\n",
      "0.8588517\n",
      "0.8564593\n",
      "0.8660287\n",
      "0.8516746\n",
      "0.8564593\n",
      "0.8516746\n",
      "0.8588517\n",
      "0.8444976\n",
      "0.85406697\n",
      "0.8564593\n",
      "0.8516746\n",
      "0.8564593\n",
      "0.85406697\n",
      "0.8492823\n",
      "0.8444976\n",
      "0.8516746\n",
      "0.8492823\n",
      "0.8516746\n",
      "0.84689\n",
      "0.8564593\n",
      "0.8444976\n",
      "0.8564593\n",
      "0.8492823\n",
      "0.861244\n",
      "0.8516746\n",
      "0.8588517\n",
      "0.8492823\n",
      "0.85406697\n",
      "0.8444976\n",
      "0.8564593\n",
      "0.84689\n",
      "0.8516746\n",
      "0.8492823\n",
      "0.8516746\n",
      "0.85406697\n",
      "0.8516746\n",
      "0.85406697\n",
      "0.85406697\n",
      "0.8516746\n",
      "0.8516746\n",
      "0.8516746\n",
      "0.8492823\n",
      "0.8564593\n",
      "0.8444976\n",
      "0.85406697\n",
      "0.8492823\n",
      "0.8636364\n",
      "0.84210527\n",
      "0.8660287\n",
      "0.84210527\n",
      "0.8636364\n",
      "0.8516746\n",
      "0.8564593\n",
      "0.8516746\n",
      "0.8444976\n",
      "0.85406697\n",
      "0.8516746\n",
      "0.861244\n",
      "0.8492823\n",
      "0.8564593\n",
      "0.84689\n",
      "0.8492823\n",
      "0.8516746\n",
      "0.8444976\n",
      "0.8588517\n",
      "0.84689\n",
      "0.8588517\n",
      "0.84689\n",
      "0.8492823\n",
      "0.84689\n",
      "0.84689\n",
      "0.8588517\n",
      "0.8492823\n",
      "0.861244\n",
      "0.84689\n",
      "0.85406697\n",
      "0.8516746\n",
      "0.84689\n",
      "0.861244\n",
      "0.8492823\n",
      "0.861244\n",
      "0.84689\n",
      "0.8564593\n",
      "0.8492823\n",
      "0.85406697\n",
      "0.8516746\n",
      "0.8492823\n",
      "0.8564593\n",
      "0.8492823\n",
      "0.8588517\n",
      "0.8444976\n",
      "0.8516746\n",
      "0.84210527\n",
      "0.84689\n",
      "0.8516746\n",
      "0.84689\n",
      "0.8588517\n",
      "0.8492823\n",
      "0.8516746\n",
      "0.84689\n",
      "0.8516746\n",
      "0.8444976\n",
      "0.8492823\n",
      "0.84210527\n",
      "0.84689\n",
      "0.84689\n",
      "0.8444976\n",
      "0.8516746\n",
      "0.8444976\n",
      "0.84689\n",
      "0.8397129\n",
      "0.8516746\n",
      "0.8444976\n",
      "0.8516746\n",
      "0.8492823\n",
      "0.8564593\n",
      "0.8444976\n",
      "0.8636364\n",
      "0.8444976\n",
      "0.8660287\n",
      "0.8444976\n",
      "0.8636364\n",
      "0.8444976\n",
      "0.8564593\n",
      "0.8444976\n",
      "0.8444976\n",
      "0.84689\n",
      "0.8444976\n",
      "0.8564593\n",
      "0.8444976\n",
      "0.8636364\n",
      "0.84210527\n",
      "0.8660287\n",
      "0.8444976\n",
      "0.8588517\n",
      "0.84210527\n",
      "0.85406697\n",
      "0.8444976\n",
      "0.8444976\n",
      "0.8444976\n",
      "0.84210527\n",
      "0.8516746\n",
      "0.84210527\n",
      "0.85406697\n",
      "0.8444976\n",
      "0.8636364\n",
      "0.8444976\n",
      "0.861244\n",
      "0.84689\n",
      "0.8588517\n",
      "0.84210527\n",
      "0.85406697\n",
      "0.84210527\n",
      "0.8492823\n",
      "0.84210527\n",
      "0.8492823\n",
      "0.84210527\n",
      "0.8444976\n",
      "0.8444976\n",
      "0.8397129\n",
      "0.84210527\n",
      "0.8397129\n",
      "0.84210527\n",
      "0.83732057\n",
      "0.84689\n",
      "0.84210527\n",
      "0.8516746\n",
      "0.84210527\n",
      "0.85406697\n",
      "0.8444976\n",
      "0.8588517\n",
      "0.84210527\n",
      "0.87081337\n",
      "0.84210527\n",
      "0.87081337\n",
      "0.84210527\n",
      "0.8660287\n",
      "0.84689\n",
      "0.84689\n",
      "0.8492823\n",
      "0.8444976\n",
      "0.8636364\n",
      "0.8444976\n",
      "0.8636364\n",
      "0.84210527\n",
      "0.8516746\n",
      "0.8397129\n",
      "0.8444976\n",
      "0.85406697\n",
      "0.8444976\n",
      "0.8660287\n",
      "0.8444976\n",
      "0.8588517\n",
      "0.8444976\n",
      "0.8444976\n",
      "0.8444976\n",
      "0.8444976\n",
      "0.8564593\n",
      "0.84689\n",
      "0.8588517\n",
      "0.8444976\n",
      "0.85406697\n",
      "0.8444976\n",
      "0.8444976\n",
      "0.84210527\n",
      "0.84210527\n",
      "0.84210527\n",
      "0.8444976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8492823\n",
      "0.84689\n",
      "0.8516746\n",
      "0.84689\n",
      "0.85406697\n",
      "0.8444976\n",
      "0.8588517\n",
      "0.84689\n",
      "0.8564593\n",
      "0.8444976\n",
      "0.8564593\n",
      "0.84689\n",
      "0.8588517\n",
      "0.84689\n",
      "0.8516746\n",
      "0.84689\n",
      "0.861244\n",
      "0.8444976\n",
      "0.85406697\n",
      "0.84210527\n",
      "0.85406697\n",
      "0.84210527\n",
      "0.85406697\n",
      "0.8444976\n",
      "0.8516746\n",
      "0.8397129\n",
      "0.8564593\n",
      "0.8444976\n",
      "0.8588517\n",
      "0.8444976\n",
      "0.8636364\n",
      "0.84210527\n",
      "0.861244\n",
      "0.8492823\n",
      "0.8588517\n",
      "0.84210527\n",
      "0.861244\n",
      "0.8397129\n",
      "0.8684211\n",
      "0.84689\n",
      "0.8660287\n",
      "0.8444976\n",
      "0.8564593\n",
      "0.84210527\n",
      "0.85406697\n",
      "0.8397129\n",
      "0.8564593\n",
      "0.84210527\n",
      "0.8492823\n",
      "0.84210527\n",
      "0.8492823\n",
      "0.8444976\n",
      "0.85406697\n",
      "0.84210527\n",
      "0.861244\n",
      "0.84689\n",
      "0.8636364\n",
      "0.8492823\n",
      "0.861244\n",
      "0.84210527\n",
      "0.8636364\n",
      "0.84210527\n",
      "0.8660287\n",
      "0.84210527\n",
      "0.8636364\n",
      "0.84210527\n",
      "0.8564593\n",
      "0.84210527\n",
      "0.85406697\n",
      "0.8444976\n",
      "0.8492823\n",
      "0.8444976\n",
      "0.84210527\n",
      "0.84210527\n",
      "0.8444976\n",
      "0.8397129\n",
      "0.8444976\n",
      "0.84210527\n",
      "0.85406697\n",
      "0.8397129\n",
      "0.85406697\n",
      "0.84210527\n",
      "0.8588517\n",
      "0.8444976\n",
      "0.87081337\n",
      "0.8397129\n",
      "0.87081337\n",
      "0.84210527\n",
      "0.87081337\n",
      "0.8349282\n",
      "0.8492823\n",
      "0.8492823\n",
      "0.84210527\n",
      "0.861244\n",
      "0.8444976\n",
      "0.8636364\n",
      "0.84210527\n",
      "0.861244\n",
      "0.84210527\n",
      "0.84210527\n",
      "0.8444976\n",
      "0.84210527\n",
      "0.8564593\n",
      "0.8444976\n",
      "0.861244\n",
      "0.84210527\n",
      "0.85406697\n",
      "0.84210527\n",
      "0.8444976\n",
      "0.84210527\n",
      "0.8397129\n",
      "0.84689\n",
      "0.84210527\n",
      "0.85406697\n",
      "0.8397129\n",
      "0.8492823\n",
      "0.8397129\n",
      "0.8564593\n",
      "0.8397129\n",
      "0.8492823\n",
      "0.84210527\n",
      "0.8492823\n",
      "0.84210527\n",
      "0.8516746\n",
      "0.83732057\n",
      "0.85406697\n",
      "0.84210527\n",
      "0.8492823\n",
      "0.84210527\n",
      "0.84210527\n",
      "0.8444976\n",
      "0.8588517\n",
      "0.8397129\n",
      "0.8516746\n",
      "0.8397129\n",
      "0.8516746\n",
      "0.84210527\n",
      "0.85406697\n",
      "0.8397129\n",
      "0.85406697\n",
      "0.84210527\n",
      "0.8492823\n",
      "0.84689\n",
      "0.8564593\n",
      "0.83732057\n",
      "0.85406697\n",
      "0.8349282\n",
      "0.8516746\n",
      "0.8397129\n",
      "0.8516746\n",
      "0.8397129\n",
      "0.8492823\n",
      "0.8397129\n",
      "0.84689\n",
      "0.84210527\n",
      "0.84689\n",
      "0.83732057\n",
      "0.8516746\n",
      "0.8444976\n",
      "0.8516746\n",
      "0.83732057\n",
      "0.85406697\n",
      "0.83732057\n",
      "0.8588517\n",
      "0.8444976\n",
      "0.8588517\n",
      "0.8444976\n",
      "0.861244\n",
      "0.8444976\n",
      "0.8636364\n",
      "0.8444976\n",
      "0.8588517\n",
      "0.84210527\n",
      "0.8564593\n",
      "0.8397129\n",
      "0.8492823\n",
      "0.84210527\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(epoch_n):\n",
    "        sess.run(optimize, feed_dict = {x : train_x, y : train_y})\n",
    "        \n",
    "        acc_p = sess.run(acc, feed_dict = {x: test_x, y : test_y })\n",
    "        print(acc_p)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'manual weight and bias tutorial : https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/neural_network_raw.py'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"manual weight and bias tutorial : https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/neural_network_raw.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_1 = 100\n",
    "hidden_2 = 200\n",
    "\n",
    "input_shape = 18\n",
    "pred_class = 2\n",
    "\n",
    "x_ = tf.placeholder('float', shape=[None,input_shape])\n",
    "y_ = tf.placeholder('float', shape=[None,pred_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'w1' : tf.Variable(tf.random_normal([input_shape, hidden_1])),\n",
    "    'w2' : tf.Variable(tf.random_normal([hidden_1, hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([hidden_2, pred_class]))\n",
    "}\n",
    "\n",
    "bias = {\n",
    "    'b1' : tf.Variable(tf.random_normal([hidden_1])),\n",
    "    'b2' : tf.Variable(tf.random_normal([hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([pred_class]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_model(x):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), bias['b1'])\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), bias['b2'])\n",
    "    out_layer = tf.add(tf.matmul(layer_2, weights['out']), bias['out'])\n",
    "    \n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = neural_model(x_)\n",
    "\n",
    "\n",
    "loss_ = tf.nn.softmax_cross_entropy_with_logits(labels=train_y, logits=logistic)\n",
    "optimize_ = tf.train.AdamOptimizer(0.001).minimize(loss_)\n",
    "corr_pred_ = tf.equal(tf.argmax(logistic,axis=1), tf.argmax(y_, axis=1))\n",
    "acc_ = tf.reduce_mean(tf.cast(corr_pred_, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(418, 18)\n",
      "(418, 2)\n",
      "(891, 18)\n",
      "(891, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(test_x))\n",
    "print(np.shape(test_y))\n",
    "\n",
    "print(np.shape(train_x))\n",
    "print(np.shape(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 accuracy 0.4712918698787689\n",
      "Epoch 50 accuracy 0.7870813608169556\n",
      "Epoch 100 accuracy 0.8181818127632141\n",
      "Epoch 150 accuracy 0.8636363744735718\n",
      "Epoch 200 accuracy 0.8827751278877258\n",
      "Epoch 250 accuracy 0.9043062329292297\n",
      "Epoch 300 accuracy 0.7440191507339478\n",
      "Epoch 350 accuracy 0.8971291780471802\n",
      "Epoch 400 accuracy 0.8708133697509766\n",
      "Epoch 450 accuracy 0.7822966575622559\n",
      "Epoch 500 accuracy 0.8875598311424255\n",
      "Epoch 550 accuracy 0.8444976210594177\n",
      "Epoch 600 accuracy 0.7511961460113525\n",
      "Epoch 650 accuracy 0.6698564887046814\n",
      "Epoch 700 accuracy 0.8947368264198303\n",
      "Epoch 750 accuracy 0.5\n",
      "Epoch 800 accuracy 0.9258373379707336\n",
      "Epoch 850 accuracy 0.7248803973197937\n",
      "Epoch 900 accuracy 0.9138755798339844\n",
      "Epoch 950 accuracy 0.8684210777282715\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(1000):\n",
    "        t = sess.run(optimize_, feed_dict = {x_ : train_x, y_ : train_y})\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            acc_p = sess.run(acc_, feed_dict = {x_: test_x, y_ : test_y })\n",
    "            print('Epoch {} accuracy {}'.format(i, acc_p))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
